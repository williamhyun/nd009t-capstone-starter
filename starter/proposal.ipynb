{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Counting on Amazon Bin Image Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background\n",
    "\n",
    "Supply chain management is essential in economics as it allows enterprises to:\n",
    "1. Achieve better resource monitoring and planning\n",
    "2. Eliminate redundant transportation costs\n",
    "3. Satisify customers to give a trace-ability for their delivery\n",
    "\n",
    "Since [the invention of the Barcode and its patent in 1951](https://en.wikipedia.org/wiki/Barcode), it has been used for inventory managements and still useful as we experience at our daily lives like in-store checkouts. However, reading barcodes printed on the products is time-consuming because we need to find the exact location of the barcodes. At every checkpoint, reading barcodes costs lots of human-labors.\n",
    "\n",
    "As a technically better solution, the first ancestor of modern [RFID](https://en.wikipedia.org/wiki/Radio-frequency_identification) was patented in 1973. Like [FasTrak](https://www.bayareafastrak.org/) on highways, RFID tags can be read automatically without human-intervention. However, RFID tags are more expensive than the printed barcodes so that they increase the final product cost. In addition, RFID tags have environmental side-effects becuase they are microchips which produces electronic wastes. Due to the limitation, they are often attached manually to the assets during the tracking period and detached to be reused for other assets later.\n",
    "\n",
    "During the last decade, image classification using machine learning has been significantly improved a lot from `50%` to `90%` in terms of the classification accuracy.\n",
    "\n",
    "https://paperswithcode.com/sota/image-classification-on-imagenet\n",
    "\n",
    "![ImageNet](https://github.com/williamhyun/nd009t-capstone-starter/raw/70e67acc83d433ed6945a4d813b77590a0e060d5/starter/ImageNet.png)\n",
    "\n",
    "The improved vision-based machine learning can disrupt the existing supply chain operating practices by introducing new ways to save costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem Statement\n",
    "\n",
    "Although there exists many published pre-trained models based on `ImageNet`, it is not easy to apply in the real-world problems due to the lack of resources like real-domain data or computing resources. To achieve a reasonably good accuracy within the given project timeline in the real-world environment, we need to narrow down the focus.\n",
    "\n",
    "Amazon provides a good real-world image data set for object recognition and counting problem. As one of the simplified version of object recognition, this project focuses on `Object Counting` problem on Amazon Bin Image Dataset instead of identifing the objects by reading barcode or RFID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Datasets: Amazon Bin Image Dataset\n",
    "\n",
    "The Amazon Bin Image Dataset contains over 500,000 images and metadata from bins of a pod in an operating Amazon Fulfillment Center. The bin images in this dataset are captured as robot units carry pods as part of normal Amazon Fulfillment Center operations. This dataset has many images and the corresponding medadata.\n",
    "\n",
    "### 3.1 Documentation\n",
    "- https://github.com/awslabs/open-data-docs/tree/main/docs/aft-vbi-pds\n",
    "\n",
    "### 3.2 Download\n",
    "- https://registry.opendata.aws/amazon-bin-imagery/\n",
    "- https://github.com/awslabs/open-data-registry/blob/main/datasets/amazon-bin-imagery.yaml\n",
    "\n",
    "### 3.3 License\n",
    "\n",
    "> Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States (CC BY-NC-SA 3.0 US) https://creativecommons.org/licenses/by-nc-sa/3.0/us/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Metadata\n",
    "\n",
    "Metadata files are JSON files containing `image_fname` and `EXPECTED_QUANTITY` fields that can be useful in this counting capstone project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"BIN_FCSKU_DATA\": {},\n",
      "    \"EXPECTED_QUANTITY\": 0,\n",
      "    \"image_fname\": \"1.jpg\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --no-sign-request s3://aft-vbi-pds/metadata/1.json -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains 536,434 **JSON** metadata files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --no-sign-request s3://aft-vbi-pds/metadata/ --summarize > metadatalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-21 18:27:38          0 \n",
      "2017-01-13 15:06:53       2472 00001.json\n"
     ]
    }
   ],
   "source": [
    "!head -n2 metadatalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-13 17:59:16       4529 99993.json\n",
      "2017-01-13 17:59:16       4529 99994.json\n",
      "2017-01-13 17:59:16       3740 99995.json\n",
      "2017-01-13 17:59:16        864 99996.json\n",
      "2017-01-13 17:59:16       2132 99997.json\n",
      "2017-01-13 17:59:16       2770 99998.json\n",
      "2017-01-13 17:59:16       1658 99999.json\n",
      "\n",
      "Total Objects: 536435\n",
      "   Total Size: 1098414519\n"
     ]
    }
   ],
   "source": [
    "!tail metadatalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Data Size\n",
    "\n",
    "It's useful to check the datasize to get the idea of data processing cost. The dataset has 536,434 **JPEG** files in total in align with metadata files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --no-sign-request s3://aft-vbi-pds/bin-images/ --summarize > list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-17 10:28:56          0 \n",
      "2017-01-13 14:47:53      45769 00001.jpg\n"
     ]
    }
   ],
   "source": [
    "!head -n2 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-13 18:03:14      80192 99993.jpg\n",
      "2017-01-13 18:03:14     104201 99994.jpg\n",
      "2017-01-13 18:03:14     103665 99995.jpg\n",
      "2017-01-13 18:03:14      58212 99996.jpg\n",
      "2017-01-13 18:03:14      39300 99997.jpg\n",
      "2017-01-13 18:03:14      36076 99998.jpg\n",
      "2017-01-13 18:03:14      35218 99999.jpg\n",
      "\n",
      "Total Objects: 536435\n",
      "   Total Size: 30466377489\n"
     ]
    }
   ],
   "source": [
    "!tail list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 File Naming Rules\n",
    "\n",
    "The dataset considers `1.jpg` and `00001.jpg` differently like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-17 17:30:24      56301 1.jpg\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --no-sign-request s3://aft-vbi-pds/bin-images/1.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-13 22:47:53      45769 00001.jpg\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --no-sign-request s3://aft-vbi-pds/bin-images/00001.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, it has 536,434 images:\n",
    "- **1~4 digit**: `1.jpg` ~ `1200.jpg`: 1200\n",
    "- **5-digit**: `00001.jpg` ~ `99999.jpg`: 99,999\n",
    "- **6-digit**: `100000.jpg` ~ `535234.jpg`: 435,235"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addtion, in `Udacity Capston preparation chapter`, `Project Overview: Inventory Monitoring at Distribution Centers` provides a selected subset of files, `file_list.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis\n",
    "\n",
    "It's difficult to analyze all 536,434 image data. In this section, I'll focus on the following three.\n",
    "- `Group 1`: 1200 files with 1~4 digits in their file names\n",
    "- `Group 2`: 99,999 files with 5 digits in their file names\n",
    "- `Group 3`: 10,441 flles in the given `file_list.json`\n",
    "\n",
    "### 4.1 `Group 1` with 1~4 digit in file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [01:27<00:00, 13.72it/s]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_metadata():\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    directory = 'metadata'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    for i in tqdm(range(1, 1201)):\n",
    "        file_name = \"%s.json\" % i\n",
    "        s3_client.download_file('aft-vbi-pds', os.path.join(directory, file_name), os.path.join(directory, file_name))\n",
    "\n",
    "download_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      n\n",
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "...  ..\n",
       "1195  5\n",
       "1196  5\n",
       "1197  5\n",
       "1198  5\n",
       "1199  5\n",
       "\n",
       "[1200 rows x 1 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "values = []\n",
    "directory = \"metadata\"\n",
    "for i in range(1, 1201):\n",
    "    filename = \"%s/%s.json\" % (directory, i)\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        n = data['EXPECTED_QUANTITY']\n",
    "        values.append(n)\n",
    "\n",
    "df = pd.DataFrame({'n': values})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.708537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 n\n",
       "count  1200.000000\n",
       "mean      2.500000\n",
       "std       1.708537\n",
       "min       0.000000\n",
       "25%       1.000000\n",
       "50%       2.500000\n",
       "75%       4.000000\n",
       "max       5.000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'n'}>]], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Group 1` is evenly distributed like a synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 `Group 2` with 5 digits in file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_metadata():\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    directory = 'metadata'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    for i in tqdm(range(1, 100000)):\n",
    "        file_name = \"%05d.json\" % i\n",
    "        s3_client.download_file('aft-vbi-pds', os.path.join(directory, file_name), os.path.join(directory, file_name))\n",
    "\n",
    "download_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "values = []\n",
    "directory = \"metadata\"\n",
    "for i in range(1, 100000):\n",
    "    filename = \"%s/%05d.json\" % (directory, i)\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        n = data['EXPECTED_QUANTITY']\n",
    "        values.append(n)\n",
    "\n",
    "df = pd.DataFrame({'n': values})\n",
    "df.to_csv(\"00001-99999.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99999 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        n\n",
       "0      12\n",
       "1      17\n",
       "2      16\n",
       "3       5\n",
       "4       4\n",
       "...    ..\n",
       "99994   5\n",
       "99995   2\n",
       "99996  16\n",
       "99997   5\n",
       "99998   6\n",
       "\n",
       "[99999 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"00001-99999.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>99999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.125321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.758112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>209.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  n\n",
       "count  99999.000000\n",
       "mean       5.125321\n",
       "std        4.758112\n",
       "min        0.000000\n",
       "25%        3.000000\n",
       "50%        4.000000\n",
       "75%        6.000000\n",
       "max      209.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`95%` data belongs to 0 ~ 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n    12.0\n",
       "Name: 0.95, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.quantile(.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'n'}>]], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist(bins=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 `Group 3` with the given file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 has 1228 items\n",
      "2 has 2299 items\n",
      "3 has 2666 items\n",
      "4 has 2373 items\n",
      "5 has 1875 items\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('file_list.json', 'r') as f:\n",
    "    d=json.load(f)\n",
    "for k, v in d.items():\n",
    "    print(\"%s has %d items\" % (k, len(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, `file_list.json` has 10,441 files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 EDA Conclusion\n",
    "In general, `Group 1` is evenly distributed but the size is too small because it has only 200 images per group. `Group 2` provides more realistic dataset but the distribution is skewed. `Group 3` `file_list.json` has more evenly distributed files. `file_list.json` is a good start to train the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Proposed Solution\n",
    "\n",
    "I propose to build an object counting service on AWS. The following is the overall architecture.\n",
    "\n",
    "![Architecture](https://github.com/williamhyun/nd009t-capstone-starter/raw/70e67acc83d433ed6945a4d813b77590a0e060d5/starter/architecture.png)\n",
    "\n",
    "- **Input**: `Amazon Bin Image Dataset` is used as the training data.\n",
    "- **Machine Learning**: Transfer learning based on pre-trained CNN Models from the PyTorch library.\n",
    "- **Training**: SageMaker is used to train PyTorch models from the training data on S3\n",
    "- **Model**: S3 is used to store the trained model\n",
    "- **Serving**: Labmda is used to serve the trained model as a service\n",
    "- **Output**: Service will count object from 0 to 12. According to the EDA result, we are focusing on 95% image data which belongs to 0 ~ 12.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Benchmark model\n",
    "\n",
    "PyTorch provides pre-trained models and `ResNet` is `Deep residual networks pre-trained on ImageNet`.\n",
    "\n",
    "- https://pytorch.org/hub/pytorch_vision_resnet/ (`ResNet18`, `ResNet34`, `ResNet50`, `ResNet101`, `ResNet152`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Network Architecture\n",
    "\n",
    "In `Deep residual networks pre-trained on ImageNet`(https://arxiv.org/pdf/1512.03385.pdf), the following network was proposed. The left is a plain network with 34 parameter layers and the right is a residual network with 34 parameter layers.\n",
    "\n",
    "![ResNet](https://github.com/williamhyun/nd009t-capstone-starter/raw/70e67acc83d433ed6945a4d813b77590a0e060d5/starter/ResNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During Udacity Capston preparation chapter, `Project Overview: Inventory Monitoring at Distribution Centers`, I built the benchmark model by adding linear fully-connected-layer at the end with the following setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net():\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False   \n",
    "\n",
    "    num_features=model.fc.in_features\n",
    "    model.fc = nn.Sequential(nn.Linear(num_features, 5))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input: 10,441 image files from `file_list.json` are used\n",
    "- Output: 1 ~ 5\n",
    "- `ResNet18` as a pre-trained model.\n",
    "- `ml.m5.xlarge` machine\n",
    "- `Spot Instance` is used and `63.8%` cost is saved\n",
    "- `Average accuracy` was `30%`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Data Preparation (Sampling based on `file_list.json`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Split into train/valid/test sets (8:1:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 has 1228 items\n",
      "982:123:123\n",
      "2 has 2299 items\n",
      "1839:230:230\n",
      "3 has 2666 items\n",
      "2132:267:267\n",
      "4 has 2373 items\n",
      "1898:237:238\n",
      "5 has 1875 items\n",
      "1500:187:188\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open('file_list.json', 'r') as f:\n",
    "    d=json.load(f)\n",
    "\n",
    "train = {}\n",
    "valid = {}\n",
    "test = {}\n",
    "for k, v in d.items():\n",
    "    l = len(v)\n",
    "    print(\"%s has %d items\" % (k, l))\n",
    "    random.shuffle(v)\n",
    "    i = (int)(0.8 * l)\n",
    "    j = (int)(0.9 * l)\n",
    "    train[k] = v[:i]\n",
    "    valid[k] = v[i:j]\n",
    "    test[k] = v[j:]\n",
    "    print(\"%s:%s:%s\" % (len(train[k]), len(valid[k]), len(test[k])))\n",
    "    \n",
    "with open('train.json', 'w') as f:\n",
    "    json.dump(train, f)\n",
    "with open('valid.json', 'w') as f:\n",
    "    json.dump(valid, f)\n",
    "with open('test.json', 'w') as f:\n",
    "    json.dump(test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Download and organize data\n",
    "\n",
    "Downloads training data and arranges it in subfolders. Each of these subfolders contain images where the number of objects is equal to the name of the folder.\n",
    "\n",
    "```\n",
    "$ tree .\n",
    ".\n",
    "├── test\n",
    "│   ├── 1\n",
    "│   ├── 2\n",
    "│   ├── 3\n",
    "│   ├── 4\n",
    "│   └── 5\n",
    "├── train\n",
    "│   ├── 1\n",
    "│   ├── 2\n",
    "│   ├── 3\n",
    "│   ├── 4\n",
    "│   └── 5\n",
    "└── valid\n",
    "    ├── 1\n",
    "    ├── 2\n",
    "    ├── 3\n",
    "    ├── 4\n",
    "    └── 5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_and_arrange_data(folder):\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    with open(folder + '.json', 'r') as f:\n",
    "        d=json.load(f)\n",
    "\n",
    "    for k, v in d.items():\n",
    "        print(f\"Downloading Images with {k} objects\")\n",
    "        directory=os.path.join(folder, k)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        for file_path in tqdm(v):\n",
    "            file_name=os.path.basename(file_path).split('.')[0]+'.jpg'\n",
    "            s3_client.download_file('aft-vbi-pds', os.path.join('bin-images', file_name),\n",
    "                             os.path.join(directory, file_name))\n",
    "\n",
    "download_and_arrange_data('train')\n",
    "download_and_arrange_data('valid')\n",
    "download_and_arrange_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:\n",
      "total 52\n",
      "drwxrwxr-x 7 ec2-user ec2-user  4096 Apr 20 23:58 .\n",
      "drwxrwxr-x 7 ec2-user ec2-user  4096 Apr 21 05:19 ..\n",
      "drwxrwxr-x 2 ec2-user ec2-user  4096 Apr 20 23:57 1\n",
      "drwxrwxr-x 2 ec2-user ec2-user 12288 Apr 20 23:57 2\n",
      "drwxrwxr-x 2 ec2-user ec2-user 12288 Apr 20 23:57 3\n",
      "drwxrwxr-x 2 ec2-user ec2-user 12288 Apr 20 23:58 4\n",
      "drwxrwxr-x 2 ec2-user ec2-user  4096 Apr 20 23:58 5\n",
      "\n",
      "train:\n",
      "total 284\n",
      "drwxrwxr-x 7 ec2-user ec2-user  4096 Apr 21 00:12 .\n",
      "drwxrwxr-x 7 ec2-user ec2-user  4096 Apr 21 05:19 ..\n",
      "drwxrwxr-x 2 ec2-user ec2-user 36864 Apr 21 00:04 1\n",
      "drwxrwxr-x 2 ec2-user ec2-user 65536 Apr 21 00:07 2\n",
      "drwxrwxr-x 2 ec2-user ec2-user 69632 Apr 21 00:10 3\n",
      "drwxrwxr-x 2 ec2-user ec2-user 69632 Apr 21 00:12 4\n",
      "drwxrwxr-x 2 ec2-user ec2-user 40960 Apr 21 00:14 5\n",
      "\n",
      "valid:\n",
      "total 52\n",
      "drwxrwxr-x 7 ec2-user ec2-user  4096 Apr 21 00:02 .\n",
      "drwxrwxr-x 7 ec2-user ec2-user  4096 Apr 21 05:19 ..\n",
      "drwxrwxr-x 2 ec2-user ec2-user  4096 Apr 20 23:59 1\n",
      "drwxrwxr-x 2 ec2-user ec2-user 12288 Apr 20 23:59 2\n",
      "drwxrwxr-x 2 ec2-user ec2-user 12288 Apr 21 00:00 3\n",
      "drwxrwxr-x 2 ec2-user ec2-user 12288 Apr 21 00:01 4\n",
      "drwxrwxr-x 2 ec2-user ec2-user  4096 Apr 21 00:02 5\n"
     ]
    }
   ],
   "source": [
    "!ls -al train valid test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112M\ttrain/4\n",
      "86M\ttrain/5\n",
      "53M\ttrain/1\n",
      "107M\ttrain/2\n",
      "129M\ttrain/3\n",
      "485M\ttrain\n",
      "15M\tvalid/4\n",
      "12M\tvalid/5\n",
      "6.9M\tvalid/1\n",
      "14M\tvalid/2\n",
      "17M\tvalid/3\n",
      "62M\tvalid\n",
      "14M\ttest/4\n",
      "12M\ttest/5\n",
      "6.3M\ttest/1\n",
      "14M\ttest/2\n",
      "16M\ttest/3\n",
      "61M\ttest\n"
     ]
    }
   ],
   "source": [
    "!du -h train valid test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 Upload to S3 for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync train s3://amazonbin/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync valid s3://amazonbin/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync test s3://amazonbin/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Train the benchmark model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 `train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import smdebug.pytorch as smd\n",
    "\n",
    "logger=logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "\n",
    "def test(model, test_loader, hook):\n",
    "    model.eval()\n",
    "    running_corrects=0\n",
    "    for (inputs, labels) in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "    total_acc = running_corrects / len(test_loader.dataset)\n",
    "    logger.info(f\"Test set: Average accuracy: {100*total_acc}%\")\n",
    "    \n",
    "\n",
    "def train(model, train_loader, valid_loader, epochs, criterion, optimizer, hook):\n",
    "    count = 0\n",
    "    for e in range(epochs):\n",
    "        print(e)\n",
    "        model.train()\n",
    "        # hook.set_mode(smd.modes.TRAIN)\n",
    "        for (inputs, labels) in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            count += len(inputs)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        # hook.set_mode(smd.modes.EVAL)\n",
    "        running_corrects=0\n",
    "        with torch.no_grad():\n",
    "            for (inputs, labels) in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_corrects += torch.sum(preds == labels.data).item()\n",
    "        total_acc = running_corrects / len(valid_loader.dataset)\n",
    "        logger.info(f\"Valid set: Average accuracy: {100*total_acc}%\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "def net():\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False   \n",
    "\n",
    "    num_features=model.fc.in_features\n",
    "    model.fc = nn.Sequential(nn.Linear(num_features, 5))\n",
    "    return model\n",
    "\n",
    "def create_data_loaders(data, batch_size, test_batch_size):\n",
    "    train_data_path = os.path.join(data, 'train')\n",
    "    test_data_path = os.path.join(data, 'test')\n",
    "    validation_data_path = os.path.join(data, 'valid')\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    trainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=train_transform)\n",
    "    validset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=test_transform)\n",
    "    testset = torchvision.datasets.ImageFolder(root=test_data_path, transform=test_transform)\n",
    "    \n",
    "    return (\n",
    "        torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True),\n",
    "        torch.utils.data.DataLoader(validset, batch_size=test_batch_size, shuffle=False),\n",
    "        torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=False))\n",
    "\n",
    "def main(args):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Running on Device {device}\")\n",
    "\n",
    "    model=net()\n",
    "    model=model.to(device)\n",
    "    loss_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.fc.parameters(), lr=args.lr)\n",
    "    \n",
    "    # hook = smd.Hook.create_from_json_file()\n",
    "    # hook.register_hook(model)\n",
    "    hook = None\n",
    "\n",
    "    train_loader, valid_loader, test_loader = create_data_loaders(args.data, args.batch_size, args.test_batch_size)\n",
    "    \n",
    "    model=train(model, train_loader, valid_loader, args.epochs, loss_criterion, optimizer, hook)\n",
    "    \n",
    "    # test(model, test_loader, hook)\n",
    "    \n",
    "    torch.save(model.cpu().state_dict(), os.path.join(args.model_dir, \"model.pth\"))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser=argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for training (default: 256)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test-batch-size\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for testing (default: 1000)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        metavar=\"N\",\n",
    "        help=\"number of epochs to train (default: 2)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\", type=float, default=0.001, metavar=\"LR\", help=\"learning rate (default: 0.001)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--momentum\", type=float, default=0.5, metavar=\"M\", help=\"SGD momentum (default: 0.5)\"\n",
    "    )\n",
    "    parser.add_argument('--data', type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    \n",
    "    args=parser.parse_args()\n",
    "    \n",
    "    logging.info(f\"Learning Rate: {args.lr}\")\n",
    "    logging.info(f\"Momentum: {args.momentum}\")\n",
    "    logging.info(f\"Batch Size: {args.batch_size}\")\n",
    "    logging.info(f\"Test Batch Size: {args.test_batch_size}\")\n",
    "    logging.info(f\"Epochs: {args.epochs}\")\n",
    "    \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Train Benchmark Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"amazonbin\"\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import os\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    role=role,\n",
    "    py_version='py36',\n",
    "    framework_version=\"1.8\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    use_spot_instances=True,\n",
    "    max_run=3600,\n",
    "    max_wait=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-21 04:52:50 Starting - Starting the training job...\n",
      "2022-04-21 04:52:52 Starting - Launching requested ML instancesProfilerReport-1650516770: InProgress\n",
      ".........\n",
      "2022-04-21 04:54:43 Starting - Preparing the instances for training.........\n",
      "2022-04-21 04:56:03 Downloading - Downloading input data...........................\n",
      "2022-04-21 05:00:50 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-04-21 05:01:04,214 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-04-21 05:01:04,216 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-04-21 05:01:04,225 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-04-21 05:01:07,267 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\n",
      "2022-04-21 05:01:05 Training - Training image download completed. Training in progress.\u001b[34m2022-04-21 05:01:21,805 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-04-21 05:01:21,818 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-04-21 05:01:21,828 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-04-21 05:01:21,837 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-04-21-04-52-50-494\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-170530745045/pytorch-training-2022-04-21-04-52-50-494/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-170530745045/pytorch-training-2022-04-21-04-52-50-494/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-04-21-04-52-50-494\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-170530745045/pytorch-training-2022-04-21-04-52-50-494/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py\u001b[0m\n",
      "\u001b[34m[2022-04-21 05:01:23.574 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-04-21 05:01:23.961 algo-1:27 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-04-21 05:01:24.505 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-04-21 05:01:24.505 algo-1:27 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-04-21 05:01:24.506 algo-1:27 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-04-21 05:01:24.506 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-04-21 05:01:24.507 algo-1:27 INFO hook.py:591] name:fc.0.weight count_params:2560\u001b[0m\n",
      "\u001b[34m[2022-04-21 05:01:24.507 algo-1:27 INFO hook.py:591] name:fc.0.bias count_params:5\u001b[0m\n",
      "\u001b[34m[2022-04-21 05:01:24.507 algo-1:27 INFO hook.py:593] Total Trainable Params: 2565\u001b[0m\n",
      "\u001b[34m0\u001b[0m\n",
      "\u001b[34m[2022-04-21 05:01:24.861 algo-1:27 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-04-21 05:01:24.865 algo-1:27 INFO hook.py:488] Hook is writing from the hook with pid: 27\u001b[0m\n",
      "\u001b[34mValid set: Average accuracy: 27.68199233716475%\u001b[0m\n",
      "\u001b[34m1\u001b[0m\n",
      "\u001b[34mValid set: Average accuracy: 30.07662835249042%\u001b[0m\n",
      "\u001b[34mINFO:root:Learning Rate: 0.001\u001b[0m\n",
      "\u001b[34mINFO:root:Momentum: 0.5\u001b[0m\n",
      "\u001b[34mINFO:root:Batch Size: 32\u001b[0m\n",
      "\u001b[34mINFO:root:Test Batch Size: 32\u001b[0m\n",
      "\u001b[34mINFO:root:Epochs: 2\u001b[0m\n",
      "\u001b[34mINFO:root:Running on Device cpu\u001b[0m\n",
      "\u001b[34mDownloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0.00/44.7M [00:00<?, ?B/s]#015 60%|█████▉    | 26.8M/44.7M [00:00<00:00, 281MB/s]#015100%|██████████| 44.7M/44.7M [00:00<00:00, 289MB/s]\u001b[0m\n",
      "\u001b[34mINFO:__main__:Valid set: Average accuracy: 27.68199233716475%\u001b[0m\n",
      "\u001b[34mINFO:__main__:Valid set: Average accuracy: 30.07662835249042%\u001b[0m\n",
      "\u001b[34m2022-04-21 05:12:54,507 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-04-21 05:13:12 Uploading - Uploading generated training model\n",
      "2022-04-21 05:13:12 Completed - Training job completed\n",
      "Training seconds: 1033\n",
      "Billable seconds: 374\n",
      "Managed Spot Training savings: 63.8%\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'train': 's3://amazonbin/'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation metrics\n",
    "\n",
    "`Accuracy` is the evaluation metrics for both benchmark model and solution model.\n",
    "\n",
    "1. For comparision, 10% of images whose item count is between 0 and 5 will be selected as the final answer set.\n",
    "The benchmark model and the proposed model will be tested with that same answer set in the end.\n",
    "\n",
    "2. To measure new solution model, another 10% of images whose item count is beteen 0 and 12 will be selected as the second final answer set.\n",
    "The proposed model will be tested with that answer set in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Project Design Outline\n",
    "\n",
    "This project is already using Spot Instance to minimize the exploration cost. Given the cost saving, I will proceed the following in the order.\n",
    "\n",
    "1. First of all, use more input data to train\n",
    "2. Second, hyperparameter tuning.\n",
    "3. Thrid, consider other pre-trained CNN models like `EfficientNet`, `ResNet50`, `VGG19`, `Inceptionv3 (GoogLeNet)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 EfficientNet\n",
    "\n",
    "`EfficientNet-B1` is known to be 7.6x smaller and 5.7x faster than than `ResNet-152`.\n",
    "However, we cannot use in SageMaker.\n",
    "```\n",
    "AttributeError: module 'torchvision.models' has no attribute 'efficientnet_b0'\n",
    "```\n",
    "\n",
    "https://arxiv.org/pdf/1905.11946v5.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"amazonbin\"\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import os\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    role=role,\n",
    "    py_version='py38',\n",
    "    framework_version=\"1.8\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    use_spot_instances=True,\n",
    "    max_run=3600,\n",
    "    max_wait=3600,\n",
    ")\n",
    "\n",
    "estimator.fit({'train': 's3://amazonbin/'})"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p36",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
